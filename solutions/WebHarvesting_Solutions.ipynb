{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of WebHarvesting_Solutions.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPZRv1XlJ27QQ1xwAILpK/N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/misupova/BSSDH-2022/blob/main/solutions/WebHarvesting_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1 TASK"
      ],
      "metadata": {
        "id": "3bLNfqb6e0NF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Excercise\n",
        "\n",
        "Find the table with the list of largest cities in the EU and construct a query to import it into a Google Sheet."
      ],
      "metadata": {
        "id": "AZ2dTWhue3Ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution\n",
        "\n",
        "For that, you first need to create an empty Google Sheet.\n",
        "\n",
        "Then, find a table with the list of largest cities in the EU, for example: https://en.wikipedia.org/wiki/List_of_cities_in_the_European_Union_by_population_within_city_limits\n",
        "\n",
        "Finally, construct a formula in the Google Sheets to extract that table:\n",
        "```\n",
        "=IMPORTHTML(\"https://en.wikipedia.org/wiki/List_of_cities_in_the_European_Union_by_population_within_city_limits\", \"table\", 1)\n",
        "```\n",
        "\n",
        "Note, that in this query there should be specifically double quotes."
      ],
      "metadata": {
        "id": "Bc_FauMve40E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2 TASKS"
      ],
      "metadata": {
        "id": "HvpertyWcQP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Excercise\n",
        "\n",
        "Find out how many articles are there in the Spaceflight News API database.\n"
      ],
      "metadata": {
        "id": "iJCX6ih4cTS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution\n",
        "\n",
        "In this task I wanted you to explore documentation of this API. \n",
        "\n",
        "The documentation is available here: https://api.spaceflightnewsapi.net/v3/documentation\n",
        "\n",
        "The API call I'm asking for is: https://api.spaceflightnewsapi.net/v3/articles/count"
      ],
      "metadata": {
        "id": "3caVM9XYccDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Exercise\n",
        "\n",
        "Construct a query for the GDELT database with the following parameters: Search for articles mentioning both **fuel** and **nuclear** search terms published in Latvia, Estonia and Lithuania from Jun-01-2022 to Jun-30-2022. Retrieve the search results using Python and save results to CSV file.\n",
        "\n",
        "Hint: you can construct a query to extract data in the CSV format directly.\n"
      ],
      "metadata": {
        "id": "NvYz6p7Gc5gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution\n",
        "\n",
        "\n",
        "\n",
        "1.   Go to the GDELT API web interface: https://gdelt.github.io/\n",
        "2.   In the task I'm asking for the search term fuel and nuclear, so they should put `fuel AND nuclear` into *search term* field.\n",
        "3.   Then they should select *Latvia, Lithuania and Estonia* in the *Source Countries* fiels\n",
        "4.   Then they should select the correct data range. To be able to select the data range, the field *Recent* should be empty.\n",
        "5.   Finally, they should select either *csv* in the format field and export the data directly, or *json* field, and then convert the response to the *csv* using Pandas.\n",
        "6.   The final API call should be: `https://api.gdeltproject.org/api/v2/doc/doc?query=fuel AND nuclear%20(sourcecountry:LG%20OR%20sourcecountry:LH%20OR%20sourcecountry:EN)&mode=ArtList&maxrecords=75&format=csv&startdatetime=20220601000000&enddatetime=20220630235959`\n",
        "7.   Then they can call the API from Python as follows:\n",
        "```\n",
        "import requests\n",
        "```\n",
        "```\n",
        "base_url = 'https://api.gdeltproject.org/api/v2/'\n",
        "endpoint = 'doc/doc'\n",
        "parameters = {\n",
        "  'query': 'fuel AND nuclear (sourcecountry:LG OR sourcecountry:LH OR sourcecountry:EN)',\n",
        "  'mode': 'ArtList',\n",
        "  'maxrecords': 75,\n",
        "  'format': 'csv',\n",
        "  'startdatetime': '20220601000000',\n",
        "  'enddatetime': '20220630235959'\n",
        "}\n",
        "```\n",
        "```\n",
        "url = base_url + endpoint\n",
        "response = requests.get(url, params=parameters)\n",
        "response.json()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "M5iyTRUHc77s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3 TASKS"
      ],
      "metadata": {
        "id": "w0nWKIeFcLvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "Study the structure of any article on https://eng.lsm.lv/ website. Identify elements of the page that we might want to scrape, and their respective tags and classes."
      ],
      "metadata": {
        "id": "apAnkwhpaJck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Possible solution\n",
        "\n",
        "Example article: https://eng.lsm.lv/article/society/defense/latvia-interested-in-buying-himars-systems.a466648/\n",
        "\n",
        "Participant should solve this task using their browser's Developer Tools\n",
        "\n",
        "Example solution:\n",
        "\n",
        "*   Title: `h1` tag, `article-title` class\n",
        "*   Lead section: `h2` tag, `article-lead` class\n",
        "*   Article text: `div` tag, `article__body` class\n",
        "\n"
      ],
      "metadata": {
        "id": "zIz604u4aLZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "In this exercise, we will try to scrape Wikipedia. \n",
        "\n",
        "1.   Check the *robots.txt* file on Wikipedia. Is it allowed to scrape Wikipedia? What else is interesting in this file?\n",
        "2.   Select an article you will scrape (e.g. https://en.wikipedia.org/wiki/Lion%27s_mane_jellyfish)\n",
        "3.   Initialize a new spider (Scrapy project) called wikiSpider. There, create a\n",
        "spider article.py \n",
        "4.   Extract the title of this article\n",
        "5.   Try to add more Wikipedia pages to the `start_requests` urls and check whether your scraper works with other Wikipedia pages."
      ],
      "metadata": {
        "id": "N7VgE8MAbBkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Possible solution"
      ],
      "metadata": {
        "id": "SFh6zDF3bCZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install Scrapy\n",
        "!pip install Scrapy\n",
        "!scrapy startproject wikiSpider"
      ],
      "metadata": {
        "id": "i2Kvcww7bGnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d542cb0-b9a8-4f6d-a88c-b4167cef327d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Scrapy\n",
            "  Downloading Scrapy-2.6.2-py2.py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=16.2.0\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting zope.interface>=4.1.3\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 38.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from Scrapy) (57.4.0)\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-3.3.1-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from Scrapy) (4.2.6)\n",
            "Collecting protego>=0.1.15\n",
            "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting cryptography>=2.0\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 50.0 MB/s \n",
            "\u001b[?25hCollecting itemadapter>=0.1.0\n",
            "  Downloading itemadapter-0.6.0-py3-none-any.whl (10 kB)\n",
            "Collecting service-identity>=16.0.0\n",
            "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting cssselect>=0.9.1\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting PyDispatcher>=2.0.5\n",
            "  Downloading PyDispatcher-2.0.5.zip (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting queuelib>=1.4.2\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting parsel>=1.5.0\n",
            "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting Twisted>=17.9.0\n",
            "  Downloading Twisted-22.4.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 38.9 MB/s \n",
            "\u001b[?25hCollecting itemloaders>=1.0.1\n",
            "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
            "Collecting w3lib>=1.17.0\n",
            "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->Scrapy) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->Scrapy) (2.21)\n",
            "Collecting jmespath>=0.9.5\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.0->Scrapy) (1.15.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->Scrapy) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->Scrapy) (0.2.8)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->Scrapy) (21.4.0)\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->Scrapy) (4.1.1)\n",
            "Collecting Automat>=0.8.0\n",
            "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting incremental>=21.3.0\n",
            "  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting constantly>=15.1\n",
            "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->Scrapy) (2.10)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->Scrapy) (2.23.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->Scrapy) (3.7.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->Scrapy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->Scrapy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->Scrapy) (2022.6.15)\n",
            "Building wheels for collected packages: PyDispatcher\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11516 sha256=446df16cca9ebefceb56d5d5888f0550d618e0ec85b30e47afd4b4b4a6c4813f\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/18/21/3c6a732eaa69a339198e08bb63b7da2c45933a3428b29ec454\n",
            "Successfully built PyDispatcher\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# change working directories\n",
        "os.chdir('/content/wikiSpider/wikiSpider/spiders')"
      ],
      "metadata": {
        "id": "LX6JcwHjbSvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile article.py\n",
        "import scrapy\n",
        "\n",
        "class ArticleSpider(scrapy.Spider):\n",
        "  name='article'\n",
        "  start_urls = ['https://en.wikipedia.org/wiki/Lion%27s_mane_jellyfish',\n",
        "                'https://en.wikipedia.org/wiki/Robyn_E._Kenealy',\n",
        "                'https://en.wikipedia.org/wiki/Grodzany']\n",
        "\n",
        "  def parse(self, response):\n",
        "    title = response.css('h1.firstHeading::text').get()\n",
        "    print('Title is: ' + title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLozDDYBbI0O",
        "outputId": "8c5908ef-4951-4503-c0c5-0a54edb6aebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing article.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy runspider article.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW9byDaQcBSa",
        "outputId": "967e3e3f-f411-4c43-99ce-e063729f39a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-07-26 12:16:15 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: wikiSpider)\n",
            "2022-07-26 12:16:15 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.7.13 (default, Apr 24 2022, 01:04:09) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 37.0.4, Platform Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-07-26 12:16:15 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'wikiSpider',\n",
            " 'NEWSPIDER_MODULE': 'wikiSpider.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_LOADER_WARN_ONLY': True,\n",
            " 'SPIDER_MODULES': ['wikiSpider.spiders']}\n",
            "2022-07-26 12:16:15 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2022-07-26 12:16:15 [scrapy.extensions.telnet] INFO: Telnet Password: 9ac28542a786a8e8\n",
            "2022-07-26 12:16:15 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-07-26 12:16:15 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-07-26 12:16:15 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-07-26 12:16:15 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-07-26 12:16:15 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-07-26 12:16:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-07-26 12:16:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-07-26 12:16:16 [filelock] DEBUG: Attempting to acquire lock 139773415482256 on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-07-26 12:16:16 [filelock] DEBUG: Lock 139773415482256 acquired on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-07-26 12:16:16 [filelock] DEBUG: Attempting to acquire lock 139773430811664 on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-07-26 12:16:16 [filelock] DEBUG: Lock 139773430811664 acquired on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-07-26 12:16:16 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): publicsuffix.org:443\n",
            "2022-07-26 12:16:16 [urllib3.connectionpool] DEBUG: https://publicsuffix.org:443 \"GET /list/public_suffix_list.dat HTTP/1.1\" 200 None\n",
            "2022-07-26 12:16:16 [filelock] DEBUG: Attempting to release lock 139773430811664 on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-07-26 12:16:16 [filelock] DEBUG: Lock 139773430811664 released on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-07-26 12:16:16 [filelock] DEBUG: Attempting to release lock 139773415482256 on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-07-26 12:16:16 [filelock] DEBUG: Lock 139773415482256 released on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-07-26 12:16:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/robots.txt> (referer: None)\n",
            "2022-07-26 12:16:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Lion%27s_mane_jellyfish> (referer: None)\n",
            "2022-07-26 12:16:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Grodzany> (referer: None)\n",
            "Title is: Lion's mane jellyfish\n",
            "Title is: Grodzany\n",
            "2022-07-26 12:16:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Robyn_E._Kenealy> (referer: None)\n",
            "Title is: Robyn E. Kenealy\n",
            "2022-07-26 12:16:17 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-07-26 12:16:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 1287,\n",
            " 'downloader/request_count': 4,\n",
            " 'downloader/request_method_count/GET': 4,\n",
            " 'downloader/response_bytes': 68694,\n",
            " 'downloader/response_count': 4,\n",
            " 'downloader/response_status_count/200': 4,\n",
            " 'elapsed_time_seconds': 1.046386,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 7, 26, 12, 16, 17, 157925),\n",
            " 'httpcompression/response_bytes': 273616,\n",
            " 'httpcompression/response_count': 4,\n",
            " 'log_count/DEBUG': 15,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 74420224,\n",
            " 'memusage/startup': 74420224,\n",
            " 'response_received_count': 4,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 3,\n",
            " 'scheduler/dequeued/memory': 3,\n",
            " 'scheduler/enqueued': 3,\n",
            " 'scheduler/enqueued/memory': 3,\n",
            " 'start_time': datetime.datetime(2022, 7, 26, 12, 16, 16, 111539)}\n",
            "2022-07-26 12:16:17 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "In this exercise, you will scrape the Estonian news website: https://news.err.ee/\n",
        "\n",
        "The task is to create a spider to collect all links for the articles on the main page, then extract their title, publication date and the lead section of the article (in bold). Create a separate Scrapy project for that task. Don't forget to think about Item structure for the extracted data.\n",
        "\n",
        "Bonus task: extract also tags below the article body.\n",
        "\n"
      ],
      "metadata": {
        "id": "z1nyaMjbjF5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Possible solution"
      ],
      "metadata": {
        "id": "g1czqi00a2Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install Scrapy\n",
        "!pip install Scrapy\n",
        "!scrapy startproject errSpider"
      ],
      "metadata": {
        "id": "RlCBkFr-TAPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80510b73-1174-4c81-9439-3139f7909eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Scrapy\n",
            "  Downloading Scrapy-2.6.2-py2.py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting PyDispatcher>=2.0.5\n",
            "  Downloading PyDispatcher-2.0.5.zip (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting itemadapter>=0.1.0\n",
            "  Downloading itemadapter-0.6.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from Scrapy) (57.4.0)\n",
            "Collecting parsel>=1.5.0\n",
            "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting Twisted>=17.9.0\n",
            "  Downloading Twisted-22.4.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 50.9 MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.1\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting pyOpenSSL>=16.2.0\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting queuelib>=1.4.2\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from Scrapy) (4.2.6)\n",
            "Collecting service-identity>=16.0.0\n",
            "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting zope.interface>=4.1.3\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 11.4 MB/s \n",
            "\u001b[?25hCollecting tldextract\n",
            "  Downloading tldextract-3.3.1-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting w3lib>=1.17.0\n",
            "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting cryptography>=2.0\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 34.1 MB/s \n",
            "\u001b[?25hCollecting protego>=0.1.15\n",
            "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting itemloaders>=1.0.1\n",
            "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->Scrapy) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->Scrapy) (2.21)\n",
            "Collecting jmespath>=0.9.5\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.0->Scrapy) (1.15.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->Scrapy) (21.4.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->Scrapy) (0.2.8)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->Scrapy) (0.4.8)\n",
            "Collecting Automat>=0.8.0\n",
            "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting constantly>=15.1\n",
            "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->Scrapy) (4.1.1)\n",
            "Collecting incremental>=21.3.0\n",
            "  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->Scrapy) (2.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->Scrapy) (2.23.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->Scrapy) (3.7.1)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->Scrapy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->Scrapy) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->Scrapy) (1.24.3)\n",
            "Building wheels for collected packages: PyDispatcher\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11516 sha256=c7f66560b6123c029ef7293ea4472e8a20aa9210baf9709482a8e9b54a400791\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/18/21/3c6a732eaa69a339198e08bb63b7da2c45933a3428b29ec454\n",
            "Successfully built PyDispatcher\n",
            "Installing collected packages: w3lib, cssselect, zope.interface, requests-file, parsel, jmespath, itemadapter, incremental, hyperlink, cryptography, constantly, Automat, Twisted, tldextract, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, Scrapy\n",
            "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Scrapy-2.6.2 Twisted-22.4.0 constantly-15.1.0 cryptography-37.0.4 cssselect-1.1.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.6.0 itemloaders-1.0.4 jmespath-1.0.1 parsel-1.6.0 protego-0.2.1 pyOpenSSL-22.0.0 queuelib-1.6.2 requests-file-1.5.1 service-identity-21.1.0 tldextract-3.3.1 w3lib-1.22.0 zope.interface-5.4.0\n",
            "New Scrapy project 'errSpider', using template directory '/usr/local/lib/python3.7/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/errSpider\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd errSpider\n",
            "    scrapy genspider example example.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/errSpider/errSpider')"
      ],
      "metadata": {
        "id": "M3n7hBSuTqoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the file with Item structure definition"
      ],
      "metadata": {
        "id": "VTXASLkVYXc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile items.py\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class ErrspiderItem(scrapy.Item):\n",
        "    title = scrapy.Field()\n",
        "    lead = scrapy.Field()\n",
        "    date = scrapy.Field()\n",
        "    tags = scrapy.Field()\n"
      ],
      "metadata": {
        "id": "s3OqhSj-Twji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d440c4e-5f2a-48ac-e456-79a87ac2a7b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting items.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/errSpider/errSpider/spiders')"
      ],
      "metadata": {
        "id": "1suxLz-QUMs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the Spider file:"
      ],
      "metadata": {
        "id": "xmvceSyEYccn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile errMain.py\n",
        "import scrapy\n",
        "from errSpider.items import ErrspiderItem\n",
        "\n",
        "class ErrMainSpider(scrapy.Spider):\n",
        "  name = 'errMain'\n",
        "  allowed_domains = [\"delfi.ee\"]\n",
        "\n",
        "  start_urls = ['https://www.delfi.ee/']\n",
        "\n",
        "  def parse(self, response):\n",
        "    for href in response.css('h5.C-headline-title').css('a::attr(\"href\")'):\n",
        "      url = response.urljoin(href.get())\n",
        "      yield scrapy.Request(url, callback = self.parse_article_content)\n",
        "\n",
        "  def parse_article_content(self, response):\n",
        "    article = ErrspiderItem()\n",
        "    article['title'] = response.css('h1.C-article-info__title').get()\n",
        "    yield article"
      ],
      "metadata": {
        "id": "duq-P4awUTzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ddb5b0c-bf7c-4e54-fc2b-1474a1bc9331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting errMain.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy runspider errMain.py -o errMain.csv -t csv"
      ],
      "metadata": {
        "id": "gvg_iij9V0eG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a6c3486-ce2b-4fe0-d768-e38a08f34c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scrapy/commands/__init__.py:135: ScrapyDeprecationWarning: The -t command line option is deprecated in favor of specifying the output format within the output URI. See the documentation of the -o and -O options for more information.\n",
            "  opts.overwrite_output,\n",
            "2022-07-27 18:23:22 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: errSpider)\n",
            "2022-07-27 18:23:22 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.7.13 (default, Apr 24 2022, 01:04:09) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 37.0.4, Platform Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-07-27 18:23:22 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'errSpider',\n",
            " 'NEWSPIDER_MODULE': 'errSpider.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_LOADER_WARN_ONLY': True,\n",
            " 'SPIDER_MODULES': ['errSpider.spiders']}\n",
            "2022-07-27 18:23:22 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2022-07-27 18:23:22 [scrapy.extensions.telnet] INFO: Telnet Password: 4af5d890f7d884ca\n",
            "2022-07-27 18:23:22 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-07-27 18:23:22 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-07-27 18:23:22 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-07-27 18:23:22 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-07-27 18:23:22 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-07-27 18:23:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-07-27 18:23:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-07-27 18:23:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.delfi.ee/robots.txt> (referer: None)\n",
            "2022-07-27 18:23:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.delfi.ee/> (referer: None)\n",
            "2022-07-27 18:23:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kroonika.delfi.ee/robots.txt> (referer: None)\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ekspress.delfi.ee/robots.txt> (referer: None)\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sport.delfi.ee/robots.txt> (referer: None)\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://lemmikloom.delfi.ee/robots.txt> (referer: None)\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://lemmikloom.delfi.ee/artikkel/120041906/alguses-ei-usaldanud-nad-uldse-peremehe-poolt-unarusse-jaetud-hulkurponid-leidsid-armastava-kodu/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://ekspress.delfi.ee/artikkel/120041300/saripettur-iseteeninduses-naine-ostis-uhe-sibula-aga-viis-kaupa-koju-500-euro-eest/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://arileht.delfi.ee/robots.txt> (referer: None)\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://arileht.delfi.ee/artikkel/120042254/roomet-sormus-toiduainete-kaibemaksu-langetamine-annaks-tarbijale-olulise-voidu/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://reisijuht.delfi.ee/robots.txt> (referer: None)\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://sport.delfi.ee/artikkel/120041792/video-kannaga-loodud-iluvarav-aitas-inglismaa-naiskonna-kodusel-em-il-finaali/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://naistekas.delfi.ee/robots.txt> (referer: None)\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://epl.delfi.ee/robots.txt> (referer: None)\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://epl.delfi.ee/artikkel/120029440/tallinna-looduskaunid-narkopeidikud-tumeveebis-aritsenud-kaubamaja-eesti-filiaal-vahendas-narkootikume-meie-koigi-kaeulatuses/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://epl.delfi.ee/artikkel/120041484/kaire-uusen-teen-viis-ettepanekut-eesti-keele-kasutamise-suurendamiseks/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://lemmikloom.delfi.ee/artikkel/120041906/alguses-ei-usaldanud-nad-uldse-peremehe-poolt-unarusse-jaetud-hulkurponid-leidsid-armastava-kodu> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://ekspress.delfi.ee/artikkel/120039232/said-netist-asju-tellides-petta-see-eesti-mees-teenis-tanu-skammeritele-miljoneid/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kroonika.delfi.ee/artikkel/120042030/prints-george-kutsub-isa-williamit-toeliselt-armsa-ja-lihtsa-huudnimega> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://reisijuht.delfi.ee/artikkel/120042266/ryanairi-tootajad-plaanivad-hispaanias-iganadalaselt-streikida-kuni-jaanuarini> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://kroonika.delfi.ee/artikkel/120042030/prints-george-kutsub-isa-williamit-toeliselt-armsa-ja-lihtsa-huudnimega/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://kroonika.delfi.ee/artikkel/120042270/jaagup-kreemi-tutar-annabel-isa-on-mind-musikaalselt-vaga-palju-mojutanud/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ekspress.delfi.ee/artikkel/120042292/sojapaevik-154-paev-ukraina-esitas-hersoni-okupantidele-ultimaatumi> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://kroonika.delfi.ee/artikkel/120042238/merylin-naust-saab-vaidetavalt-taas-telestaar-olen-uks-pohitegelasi-ameerika-telesaates/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sport.delfi.ee/artikkel/120042242/itaalia-jalgpallikoondise-rundaja-siirdus-36-miljoni-euro-eest-premier-league-i> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://lemmikloom.delfi.ee/artikkel/120041906/alguses-ei-usaldanud-nad-uldse-peremehe-poolt-unarusse-jaetud-hulkurponid-leidsid-armastava-kodu>\n",
            "{'title': '<h1 class=\"C-article-info__title\">„Alguses ei usaldanud nad üldse!“ '\n",
            "          'Peremehe poolt unarusse jäetud hulkurponid leidsid armastava '\n",
            "          'kodu</h1>'}\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://arileht.delfi.ee/artikkel/120042254/roomet-sormus-toiduainete-kaibemaksu-langetamine-annaks-tarbijale-olulise-voidu> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://naistekas.delfi.ee/artikkel/120041720/eesti-mees-naise-veidrusest-kui-see-tundub-lihtne-ja-tore-siis-tegelikult-peale-esimest-kummet-minutit-viskab-kopa-ette> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://kroonika.delfi.ee/artikkel/120042030/prints-george-kutsub-isa-williamit-toeliselt-armsa-ja-lihtsa-huudnimega>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Prints George kutsub isa '\n",
            "          'Williamit tõeliselt armsa ja lihtsa hüüdnimega</h1>'}\n",
            "2022-07-27 18:23:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://reisijuht.delfi.ee/artikkel/120042266/ryanairi-tootajad-plaanivad-hispaanias-iganadalaselt-streikida-kuni-jaanuarini>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Ryanairi töötajad plaanivad '\n",
            "          'Hispaanias iganädalaselt streikida kuni jaanuarini</h1>'}\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://annestiil.delfi.ee/robots.txt> (referer: None)\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://kroonika.delfi.ee/artikkel/120042224/fotod-ookeanivaatega-poissmehemaja-brad-pitt-ostis-mitmekumne-miljoni-eest-californiasse-uue-kodu/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://ekspress.delfi.ee/artikkel/120042292/sojapaevik-154-paev-ukraina-esitas-hersoni-okupantidele-ultimaatumi>\n",
            "{'title': '<h1 class=\"C-article-info__title\">SÕJAPÄEVIK (154. päev) | Ukraina '\n",
            "          'esitas Hersoni okupantidele ultimaatumi</h1>'}\n",
            "2022-07-27 18:23:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sport.delfi.ee/artikkel/120042242/itaalia-jalgpallikoondise-rundaja-siirdus-36-miljoni-euro-eest-premier-league-i>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Itaalia jalgpallikoondise ründaja '\n",
            "          \"siirdus 36 miljoni euro eest Premier League'i</h1>\"}\n",
            "2022-07-27 18:23:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://arileht.delfi.ee/artikkel/120042254/roomet-sormus-toiduainete-kaibemaksu-langetamine-annaks-tarbijale-olulise-voidu>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Roomet Sõrmus: toiduainete '\n",
            "          'käibemaksu langetamine annaks tarbijale olulise võidu</h1>'}\n",
            "2022-07-27 18:23:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://naistekas.delfi.ee/artikkel/120041720/eesti-mees-naise-veidrusest-kui-see-tundub-lihtne-ja-tore-siis-tegelikult-peale-esimest-kummet-minutit-viskab-kopa-ette>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Eesti mees naise veidrusest: kui '\n",
            "          'see tundub lihtne ja tore, siis tegelikult peale esimest kümmet '\n",
            "          'minutit viskab kopa ette</h1>'}\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kroonika.delfi.ee/artikkel/120042270/jaagup-kreemi-tutar-annabel-isa-on-mind-musikaalselt-vaga-palju-mojutanud> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://www.delfi.ee/artikkel/120041986/foto-toompeal-kohtus-edeva-numbrimargiga-imeuhke-aston-martin-sillutisest-kerkinud-tokkepostiga-ule-200-000-euro-maksev-auto-veeti-minema/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sport.delfi.ee/artikkel/120041792/video-kannaga-loodud-iluvarav-aitas-inglismaa-naiskonna-kodusel-em-il-finaali> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://sport.delfi.ee/artikkel/120042268/fotod-levadia-laheb-ulitahtsale-euromangule-vastu-enesekindlalt-lepistu-psuhholoogiline-eelis-on-meil/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ekspress.delfi.ee/artikkel/120041300/saripettur-iseteeninduses-naine-ostis-uhe-sibula-aga-viis-kaupa-koju-500-euro-eest> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://epl.delfi.ee/artikkel/120029440/tallinna-looduskaunid-narkopeidikud-tumeveebis-aritsenud-kaubamaja-eesti-filiaal-vahendas-narkootikume-meie-koigi-kaeulatuses> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://kroonika.delfi.ee/artikkel/120042270/jaagup-kreemi-tutar-annabel-isa-on-mind-musikaalselt-vaga-palju-mojutanud>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Jaagup Kreemi tütar Annabel: isa '\n",
            "          'on mind musikaalselt väga palju mõjutanud</h1>'}\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://arileht.delfi.ee/artikkel/120041826/venelased-sokis-soomlaste-kulmast-vastuvotust-kui-nad-nagid-et-need-oleme-meie/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://omamaitse.delfi.ee/robots.txt> (referer: None)\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://omamaitse.delfi.ee/artikkel/120042162/segadused-maasikatega-fuudishi-test-kritiseerib-eesti-maasikat-maasikakasvataja-ja-teadlane-vaidlevad-vastu/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://arileht.delfi.ee/artikkel/120041998/kaspar-oja-elektri-hinnatous-eestis-on-seletamatult-suur/kommentaarid>\n",
            "2022-07-27 18:23:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sport.delfi.ee/artikkel/120041792/video-kannaga-loodud-iluvarav-aitas-inglismaa-naiskonna-kodusel-em-il-finaali>\n",
            "{'title': '<h1 class=\"C-article-info__title\">VIDEO | Kannaga löödud iluvärav '\n",
            "          'aitas Inglismaa naiskonna kodusel EM-il finaali</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://ekspress.delfi.ee/artikkel/120041300/saripettur-iseteeninduses-naine-ostis-uhe-sibula-aga-viis-kaupa-koju-500-euro-eest>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Saripettur iseteeninduses: naine '\n",
            "          'ostis ühe sibula, aga viis kaupa koju 500 euro eest</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maaleht.delfi.ee/robots.txt> (referer: None)\n",
            "2022-07-27 18:23:25 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://maaleht.delfi.ee/artikkel/120040884/lii-sammler-vooral-pollul-hernesoomine-on-lihtlabane-vargus/kommentaarid>\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://epl.delfi.ee/artikkel/120029440/tallinna-looduskaunid-narkopeidikud-tumeveebis-aritsenud-kaubamaja-eesti-filiaal-vahendas-narkootikume-meie-koigi-kaeulatuses>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Tallinna looduskaunid '\n",
            "          'narkopeidikud. Tumeveebis äritsenud kaubamaja Eesti filiaal '\n",
            "          'vahendas narkootikume meie kõigi käeulatuses</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://epl.delfi.ee/artikkel/120042264/rammsteini-kontserti-jaid-tulevases-rohepealinnas-meenutama-topsihunnikud-kas-jargmistel-suururitustel-olukord-muutub/kommentaarid>\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sport.delfi.ee/artikkel/120042268/fotod-levadia-laheb-ulitahtsale-euromangule-vastu-enesekindlalt-lepistu-psuhholoogiline-eelis-on-meil> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://arileht.delfi.ee/artikkel/120041826/venelased-sokis-soomlaste-kulmast-vastuvotust-kui-nad-nagid-et-need-oleme-meie> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://epl.delfi.ee/artikkel/120042198/saudi-kroonprints-on-euroopas-et-suhteid-parandada/kommentaarid>\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://epl.delfi.ee/artikkel/120041484/kaire-uusen-teen-viis-ettepanekut-eesti-keele-kasutamise-suurendamiseks> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://annestiil.delfi.ee/artikkel/120037838/eksklusiiv-lauren-villmanni-esimene-avameelne-intervjuu-minu-peamine-positsioon-praegu-on-olla-ema> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://www.delfi.ee/artikkel/120042126/pirita-endine-linnaosavanem-tonis-liinat-sai-arimeestelt-25-000-euro-jagu-altkaemaksu/kommentaarid>\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sport.delfi.ee/artikkel/120042268/fotod-levadia-laheb-ulitahtsale-euromangule-vastu-enesekindlalt-lepistu-psuhholoogiline-eelis-on-meil>\n",
            "{'title': '<h1 class=\"C-article-info__title\">FOTOD | Levadia läheb ülitähtsale '\n",
            "          'euromängule vastu enesekindlalt. Lepistu: psühholoogiline eelis on '\n",
            "          'meil</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://arileht.delfi.ee/artikkel/120041826/venelased-sokis-soomlaste-kulmast-vastuvotust-kui-nad-nagid-et-need-oleme-meie>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Venelased šokis soomlaste külmast '\n",
            "          'vastuvõtust: kui nad nägid, et need oleme meie...</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.delfi.ee/artikkel/120041986/foto-toompeal-kohtus-edeva-numbrimargiga-imeuhke-aston-martin-sillutisest-kerkinud-tokkepostiga-ule-200-000-euro-maksev-auto-veeti-minema> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://www.delfi.ee/artikkel/120042300/fotod-latis-liigutati-il-28-pommitajat-muuseumisse-nagu-rippuvat-mangulennukit/kommentaarid>\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ekspress.delfi.ee/artikkel/120039232/said-netist-asju-tellides-petta-see-eesti-mees-teenis-tanu-skammeritele-miljoneid> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://epl.delfi.ee/artikkel/120041484/kaire-uusen-teen-viis-ettepanekut-eesti-keele-kasutamise-suurendamiseks>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Kaire Uusen: teen viis '\n",
            "          'ettepanekut eesti keele kasutamise suurendamiseks</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://annestiil.delfi.ee/artikkel/120037838/eksklusiiv-lauren-villmanni-esimene-avameelne-intervjuu-minu-peamine-positsioon-praegu-on-olla-ema>\n",
            "{'title': '<h1 class=\"C-article-info__title\">EKSKLUSIIV | Lauren Villmanni '\n",
            "          'esimene avameelne intervjuu: minu peamine positsioon praegu on olla '\n",
            "          'ema</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://arileht.delfi.ee/artikkel/120042124/rootsi-gamestop-meditsiinifirma-aktsia-on-mustilistel-pohjustel-raketina-lendu-lainud> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https://sport.delfi.ee/artikkel/120042294/magi-oma-hooaja-lopetanud-vigastusest-millegi-sellisega-mul-varasemalt-probleeme-pole-olnud/kommentaarid>\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://epl.delfi.ee/artikkel/120042264/rammsteini-kontserti-jaid-tulevases-rohepealinnas-meenutama-topsihunnikud-kas-jargmistel-suururitustel-olukord-muutub> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kroonika.delfi.ee/artikkel/120042238/merylin-naust-saab-vaidetavalt-taas-telestaar-olen-uks-pohitegelasi-ameerika-telesaates> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.delfi.ee/artikkel/120041986/foto-toompeal-kohtus-edeva-numbrimargiga-imeuhke-aston-martin-sillutisest-kerkinud-tokkepostiga-ule-200-000-euro-maksev-auto-veeti-minema>\n",
            "{'title': '<h1 class=\"C-article-info__title\">FOTO | Toompeal kohtus edeva '\n",
            "          'numbrimärgiga imeuhke Aston Martin sillutisest kerkinud '\n",
            "          'tõkkepostiga. Üle 200 000 euro maksev auto veeti minema</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://ekspress.delfi.ee/artikkel/120039232/said-netist-asju-tellides-petta-see-eesti-mees-teenis-tanu-skammeritele-miljoneid>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Said netist asju tellides petta? '\n",
            "          'See Eesti mees teenis tänu skämmeritele miljoneid</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://epl.delfi.ee/artikkel/120041928/sojaraport-teet-kalmus-mariupol-nimetatakse-umber-sakslaste-mars-ii-d-saabusid-ule-ootuste-kiiresti> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://omamaitse.delfi.ee/artikkel/120042162/segadused-maasikatega-fuudishi-test-kritiseerib-eesti-maasikat-maasikakasvataja-ja-teadlane-vaidlevad-vastu> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://arileht.delfi.ee/artikkel/120042124/rootsi-gamestop-meditsiinifirma-aktsia-on-mustilistel-pohjustel-raketina-lendu-lainud>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Rootsi GameStop? Meditsiinifirma '\n",
            "          'aktsia on müstilistel põhjustel raketina lendu läinud</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://epl.delfi.ee/artikkel/120042264/rammsteini-kontserti-jaid-tulevases-rohepealinnas-meenutama-topsihunnikud-kas-jargmistel-suururitustel-olukord-muutub>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Rammsteini kontserti jäid '\n",
            "          'tulevases rohepealinnas meenutama topsihunnikud. Kas järgmistel '\n",
            "          'suurüritustel olukord muutub?</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://kroonika.delfi.ee/artikkel/120042238/merylin-naust-saab-vaidetavalt-taas-telestaar-olen-uks-pohitegelasi-ameerika-telesaates>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Merylin Naust saab väidetavalt '\n",
            "          'taas telestaar: olen üks põhitegelasi Ameerika telesaates</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://forte.delfi.ee/robots.txt> (referer: None)\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.delfi.ee/artikkel/120042300/fotod-latis-liigutati-il-28-pommitajat-muuseumisse-nagu-rippuvat-mangulennukit> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kroonika.delfi.ee/artikkel/120042224/fotod-ookeanivaatega-poissmehemaja-brad-pitt-ostis-mitmekumne-miljoni-eest-californiasse-uue-kodu> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://epl.delfi.ee/artikkel/120041928/sojaraport-teet-kalmus-mariupol-nimetatakse-umber-sakslaste-mars-ii-d-saabusid-ule-ootuste-kiiresti>\n",
            "{'title': '<h1 class=\"C-article-info__title\">SÕJARAPORT | Teet Kalmus: '\n",
            "          'Mariupol nimetatakse ümber. Sakslaste MARS II-d saabusid üle '\n",
            "          'ootuste kiiresti</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://omamaitse.delfi.ee/artikkel/120042162/segadused-maasikatega-fuudishi-test-kritiseerib-eesti-maasikat-maasikakasvataja-ja-teadlane-vaidlevad-vastu>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Segadused maasikatega! Fuudishi '\n",
            "          'test kritiseerib Eesti maasikat. Maasikakasvataja ja teadlane '\n",
            "          'vaidlevad vastu</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maaleht.delfi.ee/artikkel/120040884/lii-sammler-vooral-pollul-hernesoomine-on-lihtlabane-vargus> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sport.delfi.ee/artikkel/120042294/magi-oma-hooaja-lopetanud-vigastusest-millegi-sellisega-mul-varasemalt-probleeme-pole-olnud> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.delfi.ee/artikkel/120042300/fotod-latis-liigutati-il-28-pommitajat-muuseumisse-nagu-rippuvat-mangulennukit>\n",
            "{'title': '<h1 class=\"C-article-info__title\">FOTOD | Lätis liigutati IL-28 '\n",
            "          'pommitajat muuseumisse nagu rippuvat mängulennukit</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://kroonika.delfi.ee/artikkel/120042224/fotod-ookeanivaatega-poissmehemaja-brad-pitt-ostis-mitmekumne-miljoni-eest-californiasse-uue-kodu>\n",
            "{'title': '<h1 class=\"C-article-info__title\">FOTOD | Ookeanivaatega '\n",
            "          'poissmehemaja: Brad Pitt ostis mitmekümne miljoni eest '\n",
            "          'Californiasse uue kodu</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://epl.delfi.ee/artikkel/120041898/erisaade-hans-h-luik-venemaa-valmistab-avalikkust-ette-biorelva-kasutamiseks> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://maaleht.delfi.ee/artikkel/120040884/lii-sammler-vooral-pollul-hernesoomine-on-lihtlabane-vargus>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Lii Sammler: võõral põllul '\n",
            "          'hernesöömine on lihtlabane vargus</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://arileht.delfi.ee/artikkel/120041998/kaspar-oja-elektri-hinnatous-eestis-on-seletamatult-suur> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sport.delfi.ee/artikkel/120042294/magi-oma-hooaja-lopetanud-vigastusest-millegi-sellisega-mul-varasemalt-probleeme-pole-olnud>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Mägi oma hooaja lõpetanud '\n",
            "          'vigastusest: millegi sellisega mul varasemalt probleeme pole '\n",
            "          'olnud</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://epl.delfi.ee/artikkel/120041898/erisaade-hans-h-luik-venemaa-valmistab-avalikkust-ette-biorelva-kasutamiseks>\n",
            "{'title': '<h1 class=\"C-article-info__title\">ERISAADE | Hans H. Luik: Venemaa '\n",
            "          'valmistab avalikkust ette biorelva kasutamiseks</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://arileht.delfi.ee/artikkel/120041998/kaspar-oja-elektri-hinnatous-eestis-on-seletamatult-suur>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Kaspar Oja: elektri hinnatõus '\n",
            "          'Eestis on seletamatult suur</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.delfi.ee/artikkel/120042248/telegram-putin-vajas-nadalavahetusel-kiiret-arstiabi-venemaa-presidenti-voib-lahinadalatel-asendada-koopia> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://forte.delfi.ee/artikkel/120042022/video-surmajumalannad-asusid-tegevusse-ukrainasse-joudnud-eesti-droonid-aitavad-havitada-vene-tanke> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.delfi.ee/artikkel/120042248/telegram-putin-vajas-nadalavahetusel-kiiret-arstiabi-venemaa-presidenti-voib-lahinadalatel-asendada-koopia>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Telegram: Putin vajas '\n",
            "          'nädalavahetusel „kiiret arstiabi“ - Venemaa presidenti võib '\n",
            "          'lähinädalatel asendada „koopia“ </h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://forte.delfi.ee/artikkel/120042022/video-surmajumalannad-asusid-tegevusse-ukrainasse-joudnud-eesti-droonid-aitavad-havitada-vene-tanke>\n",
            "{'title': '<h1 class=\"C-article-info__title\">VIDEO | Surmajumalannad asusid '\n",
            "          'tegevusse: Ukrainasse jõudnud Eesti droonid aitavad hävitada Vene '\n",
            "          'tanke</h1>'}\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://epl.delfi.ee/artikkel/120042198/saudi-kroonprints-on-euroopas-et-suhteid-parandada> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.delfi.ee/artikkel/120042126/pirita-endine-linnaosavanem-tonis-liinat-sai-arimeestelt-25-000-euro-jagu-altkaemaksu> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://epl.delfi.ee/artikkel/120042132/intervjuu-raivo-vare-putin-passib-sugiskulmade-tulekut-et-siis-gaasikraanid-loplikult-kinni-keerata> (referer: https://www.delfi.ee/)\n",
            "2022-07-27 18:23:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://epl.delfi.ee/artikkel/120042198/saudi-kroonprints-on-euroopas-et-suhteid-parandada>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Saudi kroonprints on Euroopas, et '\n",
            "          'suhteid parandada </h1>'}\n",
            "2022-07-27 18:23:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.delfi.ee/artikkel/120042126/pirita-endine-linnaosavanem-tonis-liinat-sai-arimeestelt-25-000-euro-jagu-altkaemaksu>\n",
            "{'title': '<h1 class=\"C-article-info__title\">Pirita endine linnaosavanem Tõnis '\n",
            "          'Liinat sai ärimeestelt 25 000 euro jagu altkäemaksu</h1>'}\n",
            "2022-07-27 18:23:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://epl.delfi.ee/artikkel/120042132/intervjuu-raivo-vare-putin-passib-sugiskulmade-tulekut-et-siis-gaasikraanid-loplikult-kinni-keerata>\n",
            "{'title': '<h1 class=\"C-article-info__title\">INTERVJUU | Raivo Vare: Putin '\n",
            "          'passib sügiskülmade tulekut, et siis gaasikraanid lõplikult kinni '\n",
            "          'keerata</h1>'}\n",
            "2022-07-27 18:23:26 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-07-27 18:23:26 [scrapy.extensions.feedexport] INFO: Stored csv feed (33 items) in: errMain.csv\n",
            "2022-07-27 18:23:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/exception_count': 22,\n",
            " 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 22,\n",
            " 'downloader/request_bytes': 14829,\n",
            " 'downloader/request_count': 47,\n",
            " 'downloader/request_method_count/GET': 47,\n",
            " 'downloader/response_bytes': 4007479,\n",
            " 'downloader/response_count': 47,\n",
            " 'downloader/response_status_count/200': 47,\n",
            " 'elapsed_time_seconds': 3.820138,\n",
            " 'feedexport/success_count/FileFeedStorage': 1,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 7, 27, 18, 23, 26, 167444),\n",
            " 'httpcompression/response_bytes': 22985666,\n",
            " 'httpcompression/response_count': 47,\n",
            " 'item_scraped_count': 33,\n",
            " 'log_count/DEBUG': 103,\n",
            " 'log_count/INFO': 11,\n",
            " 'memusage/max': 75055104,\n",
            " 'memusage/startup': 75055104,\n",
            " 'request_depth_max': 1,\n",
            " 'response_received_count': 47,\n",
            " 'robotstxt/forbidden': 22,\n",
            " 'robotstxt/request_count': 13,\n",
            " 'robotstxt/response_count': 13,\n",
            " 'robotstxt/response_status_count/200': 13,\n",
            " 'scheduler/dequeued': 56,\n",
            " 'scheduler/dequeued/memory': 56,\n",
            " 'scheduler/enqueued': 56,\n",
            " 'scheduler/enqueued/memory': 56,\n",
            " 'start_time': datetime.datetime(2022, 7, 27, 18, 23, 22, 347306)}\n",
            "2022-07-27 18:23:26 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    }
  ]
}